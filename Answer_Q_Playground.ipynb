{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760d72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbc9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# model config\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline, RobertaModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model optim\n",
    "from torch.optim import AdamW, SGD\n",
    "\n",
    "# lr schedulers\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, \\\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3f0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2500a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# checkpoint -> pretrained model\n",
    "checkpoint = 'deepset/roberta-base-squad2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f4ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR encoder -> T-5 sentence dense embeddings\n",
    "encoder_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4038348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "processer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307fb7e",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ddf4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data_path for raw input and feature_path for feature input\n",
    "data_path = 'Question_Answer_Dataset_v1.2'\n",
    "feature_cache_path = 'Question_Answer_Dataset_v1.2/features_answers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01237e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features exists\n"
     ]
    }
   ],
   "source": [
    "# prepare feature data if not yet exist \n",
    "if not (os.path.exists(feature_cache_path) and os.path.isfile(feature_cache_path)):\n",
    "    # use the encoder to get the raw dataset (context are extracted by IR with the K-NN sentence to the QA pair)\n",
    "    print(\"processing raw dataset... \")\n",
    "    raw_dataset = CustomData(data_path, encoder_model, k=1)\n",
    "    print(\"computing features...\")\n",
    "    # tokenize\n",
    "    prepare_features_a(raw_dataset, feature_cache_path, processer, max_len_inp=512,max_len_out=96)\n",
    "else:\n",
    "    print(\"features exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0a0a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# feature dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# leave 425 points for testing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m test_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m425\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_cache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m FeatureData(feature_cache_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, test_points) \n",
      "File \u001b[0;32m~/question_answering/utils.py:305\u001b[0m, in \u001b[0;36mFeatureData.__init__\u001b[0;34m(self, feat_path, split, split_point)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_path \u001b[38;5;241m=\u001b[39m feat_path\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# load features\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# load the features and extract\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m-\u001b[39msplit_point]\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# feature dataset\n",
    "# leave 425 points for testing\n",
    "test_points = 425\n",
    "train_dataset = FeatureData(feature_cache_path, 'train', test_points)\n",
    "test_dataset = FeatureData(feature_cache_path, 'test', test_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82871685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what's in the dataset\n",
    "input_dict = train_dataset[0]\n",
    "print(\"input ids shape: \", input_dict['input_ids'].size())\n",
    "print(\"question ids shape: \", input_dict['target_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d08818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default split point: 425 -> samples after the split point will be in the test set\n",
    "dataloader_train, dataloader_test = get_dataloaders(train_dataset, test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[110] # this is a hard question -> chain of thoughts/verification might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50184385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, dataloader_train, n_epochs, output_dir, log_file):\n",
    "\n",
    "    model.train() # put to train mode\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "\n",
    "        losses = 0\n",
    "        for step, batch in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "\n",
    "            batch = {key: tensor.to(device) for key, tensor in batch.items()} # to cuda\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['input_mask'],\n",
    "                decoder_input_ids=batch['target_ids'],\n",
    "                decoder_attention_mask=batch['target_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            model.optimizer.zero_grad() # clear loss\n",
    "            loss.backward()\n",
    "            model.optimizer.step()  # backprop to update the weights\n",
    "\n",
    "            if model.scheduler is not None:\n",
    "                model.scheduler.step()  # update learning rate schedule \n",
    "\n",
    "            # log losses\n",
    "            loss /= len(train_dataloader)\n",
    "            losses += loss.item()\n",
    "        \n",
    "        # output stats\n",
    "        print(f\"Epoch {e}; loss {losses}\")\n",
    "        log(output_dir, \"Epoch \" + str(e+1) + \"; loss \" + str(losses), log_file)\n",
    "    \n",
    "\n",
    "def test(model, dataloader_test, output_dir, log_file):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    losses = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader_test), total=len(dataloader_test)):\n",
    "\n",
    "        batch = {key: tensor.to(device) for key, tensor in batch.items()} # to cuda \n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['input_mask'],\n",
    "            decoder_input_ids=batch['target_ids'],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # log losses\n",
    "        loss /= len(train_dataloader)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    # output stats\n",
    "    print(f\"Validation loss {losses}\")\n",
    "    log(output_dir, \"Validation loss \" + str(losses), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975f7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
