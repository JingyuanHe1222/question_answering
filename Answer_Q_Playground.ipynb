{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760d72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbc9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# model config\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline, RobertaModel, T5Config, T5ForConditionalGeneration, T5Tokenizer, T5Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model optim\n",
    "from torch.optim import AdamW, SGD\n",
    "\n",
    "# lr schedulers\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, \\\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2500a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# checkpoint -> pretrained model\n",
    "checkpoint = 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f4ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR encoder -> T-5 sentence dense embeddings\n",
    "encoder_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4038348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForQuestionAnswering.from_pretrained(checkpoint).to(device)\n",
    "# processer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1134e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and model\n",
    "processer = get_tokenizer(checkpoint)\n",
    "model = get_model(checkpoint, device, processer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307fb7e",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddf4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data_path for raw input and feature_path for feature input\n",
    "data_path = 'Question_Answer_Dataset_v1.2'\n",
    "feature_cache_path = 'Question_Answer_Dataset_v1.2/features_answers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01237e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw dataset... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1715/1715 [01:35<00:00, 17.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 826/826 [01:00<00:00, 13.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1459/1459 [01:31<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  2684\n",
      "computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2684/2684 [00:01<00:00, 1481.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare feature data if not yet exist \n",
    "if not (os.path.exists(feature_cache_path) and os.path.isfile(feature_cache_path)):\n",
    "    # use the encoder to get the raw dataset (context are extracted by IR with the K-NN sentence to the QA pair)\n",
    "    print(\"processing raw dataset... \")\n",
    "    raw_dataset = CustomData(data_path, encoder_model, k=1)\n",
    "    print(\"computing features...\")\n",
    "    # tokenize\n",
    "    prepare_features_a(raw_dataset, feature_cache_path, processer, max_len_inp=512,max_len_out=512)\n",
    "else:\n",
    "    print(\"features exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0a0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature train set:  2259\n",
      "length of feature test set:  425\n"
     ]
    }
   ],
   "source": [
    "# feature dataset\n",
    "# leave 425 points for testing\n",
    "test_points = 425\n",
    "train_dataset = FeatureData(feature_cache_path, 'train', test_points)\n",
    "test_dataset = FeatureData(feature_cache_path, 'test', test_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82871685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids shape:  torch.Size([512])\n",
      "question ids shape:  torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# check what's in the dataset\n",
    "input_dict = train_dataset[0]\n",
    "print(\"input ids shape: \", input_dict['input_ids'].size())\n",
    "print(\"question ids shape: \", input_dict['target_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98d7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[110] # this is a hard question -> chain of thoughts/verification might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ed91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'input_mask', 'target_ids', 'target_mask', 'labels'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[110].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c01da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[110]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734715f",
   "metadata": {},
   "source": [
    "### Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88501123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_model(model_dir, model, model_type='latest'):\n",
    "\n",
    "    if model_type == \"latest\":\n",
    "        saved_name = 'latest_model.pt'\n",
    "    else:\n",
    "        saved_name = 'best_model.pt'\n",
    "\n",
    "    root_model_path = os.path.join(model_dir, saved_name)\n",
    "    state_dict = {'weights': model.state_dict(), \n",
    "                  'optimizer': model.optimizer.state_dict(), \n",
    "                  'scheduler': model.scheduler.state_dict()}\n",
    "    torch.save(state_dict, root_model_path)\n",
    "        \n",
    "\n",
    "# Loads the experiment data if exists to resume training from last saved checkpoint.\n",
    "def __load_experiment(model_dir, model, model_type='latest'):\n",
    "    \n",
    "    if model_type == \"latest\":\n",
    "        saved_name = 'latest_model.pt'\n",
    "    else:\n",
    "        saved_name = 'best_model.pt'\n",
    "\n",
    "    if os.path.exists(os.path.join(model_dir, 'train.log')):\n",
    "        # get current epoch\n",
    "        current_epoch = 0\n",
    "        with open(os.path.join(model_dir, 'train.log')) as f:\n",
    "            for line in f:\n",
    "                current_epoch += 1\n",
    "        # get the latest model\n",
    "        state_dict = torch.load(os.path.join(model_dir, saved_name), map_location=device.type)\n",
    "        model.load_state_dict(state_dict['weights'])\n",
    "        model.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        model.scheduler.load_state_dict(state_dict['scheduler'])\n",
    "    else:\n",
    "        current_epoch = 0\n",
    "\n",
    "    return model, current_epoch\n",
    "\n",
    "\n",
    "def log(output_dir, log_str, file_name=None):\n",
    "    if file_name is None:\n",
    "        file_name = \"all.log\"\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(log_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50184385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, dataloader_train, n_epochs, model_dir, log_file):\n",
    "\n",
    "    model.train() # put to train mode\n",
    "    \n",
    "    # load current model if exist\n",
    "    model, current_epoch = __load_experiment(model_dir, model)\n",
    "    \n",
    "    all_losses = []\n",
    "    \n",
    "    for e in range(current_epoch, n_epochs):\n",
    "\n",
    "        losses = 0\n",
    "        for step, batch in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['input_mask'].to(device),\n",
    "                decoder_input_ids=batch['target_ids'].to(device),\n",
    "                decoder_attention_mask=batch['target_mask'].to(device), \n",
    "                labels = batch['labels'].to(device)\n",
    "            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            model.optimizer.zero_grad() # clear loss\n",
    "            loss.backward()\n",
    "            model.optimizer.step()  # backprop to update the weights\n",
    "\n",
    "            if model.scheduler is not None:\n",
    "                model.scheduler.step()  # update learning rate schedule \n",
    "\n",
    "            # log losses\n",
    "            loss /= len(dataloader_train)\n",
    "            losses += loss.item()\n",
    "            \n",
    "        # output stats\n",
    "        print(f\"Epoch {e}; loss {losses}\")\n",
    "        log(model_dir, \"Epoch \" + str(e+1) + \"; loss \" + str(losses), log_file)\n",
    "        all_losses.append(losses)\n",
    "        # save model\n",
    "        __save_model(model_dir, model) # save latest\n",
    "        if (e > current_epoch and losses < all_losses[-1]):\n",
    "            __save_model(model_dir, model, model_type='best') # save best model        \n",
    "        \n",
    "        \n",
    "\n",
    "def test(model, dataloader_test, model_dir, log_file):\n",
    "    \n",
    "    model, e = __load_experiment(model_dir, model, model_type='latest')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    losses = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader_test), total=len(dataloader_test)):\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['input_mask'].to(device),\n",
    "            decoder_input_ids=batch['target_ids'].to(device),\n",
    "            decoder_attention_mask=batch['target_mask'].to(device),\n",
    "            labels = batch['labels'].to(device)\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # log losses\n",
    "        loss /= len(dataloader_test)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    # output stats\n",
    "    print(f\"Validation loss {losses}\")\n",
    "    log(model_dir, \"Validation loss \" + str(losses), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d33bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, lr, eps): \n",
    "    if opt_name == 'Adam':\n",
    "        return AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "    elif opt_name == 'SGD':\n",
    "        return SGD(model.parameters(), lr=lr, eps=eps)\n",
    "    \n",
    "def get_scheduler(model, scheduler, n_batches, n_epochs, warmup_portion=0.1):\n",
    "    train_steps = n_epochs*n_batches\n",
    "    warm_step = int(train_steps*warmup_portion)\n",
    "    if scheduler == \"linear\": \n",
    "        return get_linear_schedule_with_warmup(model.optimizer, num_warmup_steps=warm_step,num_training_steps=train_steps)\n",
    "    elif scheduler == \"cosine\":\n",
    "        return get_cosine_schedule_with_warmup(model.optimizer, num_warmup_steps=warm_step,num_training_steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ef459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train feature data with 565 batches\n",
      "Loaded test feature data with 107 batches\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "n_epochs = 10\n",
    "lr = 1e-5\n",
    "weight_decay = 5e-5\n",
    "batch_size = 4\n",
    "\n",
    "# dataloaders\n",
    "# default split point: 425 -> samples after the split point will be in the test set\n",
    "dataloader_train, dataloader_test = get_dataloaders(train_dataset, test_dataset, batch_size=batch_size)\n",
    "\n",
    "# model optimizer\n",
    "model.optimizer = get_optimizer(model, \"Adam\", lr, weight_decay)\n",
    "\n",
    "# learning rate scheduler\n",
    "model.scheduler = get_scheduler(model, \"linear\", len(dataloader_train), n_epochs)\n",
    "\n",
    "# model state_dict\n",
    "model_dir = f\"{checkpoint}_e{n_epochs}_lr{lr}_eps{weight_decay}_Adam_linearS_batch{batch_size}\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# log file\n",
    "log_file = \"train.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cadb789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2; loss 1.0490192598044814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3; loss 0.671609889274805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4; loss 0.4829345119524078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5; loss 0.37002997546960614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6; loss 0.30222941334181996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7; loss 0.26097374822256825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8; loss 0.22786905645398292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [03:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9; loss 0.21599957597402408\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader_train, n_epochs, model_dir, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2379814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 107/107 [00:12<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.06734531256876153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloader_test, model_dir, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975f7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 3.797605GB\n",
      "torch.cuda.memory_reserved: 15.416016GB\n",
      "torch.cuda.max_memory_reserved: 15.416016GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42439d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
