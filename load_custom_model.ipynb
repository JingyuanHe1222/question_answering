{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "24b069e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.11.3\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.11.3)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n",
      "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.11.3) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.11.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2023.7.22)\n",
      "Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.3.2)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26604ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from fine_tune_answer_generation import __load_experiment, get_tokenizer, get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec1aeb",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805fdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'potsawee/t5-large-generation-squad-QuestionAnswer' # 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b908ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd902c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f5b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processer = get_tokenizer(base_model)\n",
    "model = get_model(base_model, device, processer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c2a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 't5-base_e12_lr1e-05_eps5e-05_Adam_cosine_batch1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1859c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(model_dir, 'train.log')):\n",
    "    # get current epoch\n",
    "#     current_epoch = 0\n",
    "#     with open(os.path.join(model_dir, 'train.log')) as f:\n",
    "#         for line in f:\n",
    "#             current_epoch += 1\n",
    "    # get the latest model\n",
    "    state_dict = torch.load(os.path.join(model_dir, 'latest_model.pt'), map_location=device.type)\n",
    "    model.load_state_dict(state_dict['weights'])\n",
    "else: \n",
    "    print(\"model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5c9b2",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df184328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features exists\n",
      "length of feature test set:  425\n"
     ]
    }
   ],
   "source": [
    "data_path = 'Question_Answer_Dataset_v1.2'\n",
    "feature_cache_path = 'Question_Answer_Dataset_v1.2/features_answers'\n",
    "\n",
    "# prepare feature data if not yet exist \n",
    "if not (os.path.exists(feature_cache_path) and os.path.isfile(feature_cache_path)):\n",
    "    # use the encoder to get the raw dataset (context are extracted by IR with the K-NN sentence to the QA pair)\n",
    "    print(\"processing raw dataset... \")\n",
    "    raw_dataset = CustomData(data_path, encoder_model, k=1)\n",
    "    print(\"computing features...\")\n",
    "    # tokenize\n",
    "    prepare_features_a(raw_dataset, feature_cache_path, processer, max_len_inp=512,max_len_out=512)\n",
    "else:\n",
    "    print(\"features exists\")\n",
    "\n",
    "# feature dataset\n",
    "test_dataset = FeatureData(feature_cache_path, 'test', 425) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbc639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6c152f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"The shell almost invariably has a circular opening over which the drumhead is stretched, but the shape of the remainder of the shell varies widely.\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4caacf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processer(context, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "757e515d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f7e8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22feb6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "757e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer = processer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = question_answer.replace(processer.pad_token, \"\").replace(processer.eos_token, \"\")\n",
    "question, answer = question_answer.split(processer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53f55c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What is the most common shape of  drum shell?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65345672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' drums'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9245e1",
   "metadata": {},
   "source": [
    "### answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bbe811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at t5-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model='t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d73a519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForQuestionAnswering(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e795ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What type of vegetation do giraffes prefer?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d96230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Giraffes can inhabit savannas, grasslands, or open woodlands. They prefer areas enriched with acacia growth. They drink large quantities of water and, as a result, they can spend long periods of time in dry, arid areas. When searching for more food they will venture into areas with denser foliage.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0ae172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.000419730378780514,\n",
       " 'start': 0,\n",
       " 'end': 30,\n",
       " 'answer': 'Giraffes can inhabit savannas,'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a4d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is the bear white?\"\n",
    "context = \"the bear is white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8258fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: Is the bear white? context: the bear is white'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = f\"question: {question} context: {context}\"\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa196a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0747537910938263, 'start': 0, 'end': 8, 'answer': 'the bear'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6eefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
