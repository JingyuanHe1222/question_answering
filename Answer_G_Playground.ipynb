{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760d72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbc9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# model config\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline, RobertaModel, T5Config, T5ForConditionalGeneration, T5Tokenizer, T5Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model optim\n",
    "from torch.optim import AdamW, SGD\n",
    "\n",
    "# lr schedulers\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, \\\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2500a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# checkpoint -> pretrained model\n",
    "checkpoint = \"deepset/roberta-base-squad2\" #\"t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f4ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR encoder -> T-5 sentence dense embeddings\n",
    "encoder_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb885f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processer = get_tokenizer(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c1c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(checkpoint, device, processer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307fb7e",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddf4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data_path for raw input and feature_path for feature input\n",
    "data_path = 'Question_Answer_Dataset_v1.2'\n",
    "raw_path = os.path.join(data_path, \"raw\")\n",
    "feature_cache_path = 'Question_Answer_Dataset_v1.2/features_answers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11cce485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $feature_cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24899d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE-S08,S09  \u001b[0m\u001b[01;32mREADME.v1.2\u001b[0m*  \u001b[01;34mS08\u001b[0m/  \u001b[01;34mS09\u001b[0m/  \u001b[01;34mS10\u001b[0m/  raw\r\n"
     ]
    }
   ],
   "source": [
    "ls $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01237e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw dataset... \n",
      "computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2684/2684 [00:01<00:00, 1556.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the encoder to get the raw dataset (context are extracted by IR with the K-NN sentence to the QA pair)\n",
    "print(\"processing raw dataset... \")\n",
    "if not os.path.isfile(raw_path):\n",
    "    raw_dataset = CustomData(data_path, encoder_model, k=1)\n",
    "    with open(os.path.join(data_path, \"raw\"), 'wb') as f:\n",
    "        pickle.dump(raw_dataset, f)\n",
    "else: \n",
    "    with open(os.path.join(data_path, \"raw\"), 'rb') as f:\n",
    "        raw_dataset = pickle.load(f) # deserialize using load()\n",
    "\n",
    "print(\"computing features...\")\n",
    "# prepare feature data if not yet exist \n",
    "if not (os.path.exists(feature_cache_path) and os.path.isfile(feature_cache_path)):\n",
    "    prepare_features_a(raw_dataset, feature_cache_path, processer, max_len_inp=512,max_len_out=512)\n",
    "else:\n",
    "    print(\"features exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0a0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature train set:  2259\n",
      "length of feature test set:  425\n"
     ]
    }
   ],
   "source": [
    "# feature dataset\n",
    "# leave 425 points for testing\n",
    "test_points = 425\n",
    "train_dataset = FeatureData_A(feature_cache_path, 'train', test_points)\n",
    "test_dataset = FeatureData_A(feature_cache_path, 'test', test_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82871685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids shape:  torch.Size([512])\n",
      "question ids shape:  torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# check what's in the dataset\n",
    "input_dict = train_dataset[0]\n",
    "print(\"input ids shape: \", input_dict['input_ids'].size())\n",
    "print(\"question ids shape: \", input_dict['target_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98d7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is Calvin Jr. older than John Coolidge?',\n",
       " 'No',\n",
       " 'They had two sons; John Coolidge, born in 1906, and Calvin Jr., born in 1908.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[110] # this is a hard question -> chain of thoughts/verification might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ed91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'input_mask', 'target_ids', 'target_mask', 'start', 'end', 'labels'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[110].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c01da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[110]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734715f",
   "metadata": {},
   "source": [
    "### Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88501123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_model(model_dir, model, model_type='latest'):\n",
    "\n",
    "    if model_type == \"latest\":\n",
    "        saved_name = 'latest_model.pt'\n",
    "    else:\n",
    "        saved_name = 'best_model.pt'\n",
    "\n",
    "    root_model_path = os.path.join(model_dir, saved_name)\n",
    "    state_dict = {'weights': model.state_dict(), \n",
    "                  'optimizer': model.optimizer.state_dict(), \n",
    "                  'scheduler': model.scheduler.state_dict()}\n",
    "    torch.save(state_dict, root_model_path)\n",
    "        \n",
    "\n",
    "# Loads the experiment data if exists to resume training from last saved checkpoint.\n",
    "def __load_experiment(model_dir, model, model_type='latest'):\n",
    "    \n",
    "    if model_type == \"latest\":\n",
    "        saved_name = 'latest_model.pt'\n",
    "    else:\n",
    "        saved_name = 'best_model.pt'\n",
    "\n",
    "    if os.path.exists(os.path.join(model_dir, 'train.log')):\n",
    "        # get current epoch\n",
    "        current_epoch = 0\n",
    "        with open(os.path.join(model_dir, 'train.log')) as f:\n",
    "            for line in f:\n",
    "                current_epoch += 1\n",
    "        # get the latest model\n",
    "        state_dict = torch.load(os.path.join(model_dir, saved_name), map_location=device.type)\n",
    "        model.load_state_dict(state_dict['weights'])\n",
    "        model.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        model.scheduler.load_state_dict(state_dict['scheduler'])\n",
    "    else:\n",
    "        current_epoch = 0\n",
    "\n",
    "    return model, current_epoch\n",
    "\n",
    "\n",
    "def log(output_dir, log_str, file_name=None):\n",
    "    if file_name is None:\n",
    "        file_name = \"all.log\"\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(log_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50184385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, dataloader_train, n_epochs, model_dir, log_file):\n",
    "\n",
    "    model.train() # put to train mode\n",
    "    \n",
    "    # load current model if exist\n",
    "    model, current_epoch = __load_experiment(model_dir, model)\n",
    "    \n",
    "    all_losses = []\n",
    "    \n",
    "    for e in range(current_epoch, n_epochs):\n",
    "\n",
    "        losses = 0\n",
    "        for step, batch in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['input_mask'].to(device),\n",
    "#                 decoder_input_ids=batch['target_ids'].to(device),\n",
    "#                 decoder_attention_mask=batch['target_mask'].to(device), \n",
    "                start_positions = batch['start'].to(device), \n",
    "                end_positions = batch['end'].to(device)\n",
    "            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "#             print(loss)\n",
    "\n",
    "            model.optimizer.zero_grad() # clear loss\n",
    "            loss.backward()\n",
    "            model.optimizer.step()  # backprop to update the weights\n",
    "\n",
    "            if model.scheduler is not None:\n",
    "                model.scheduler.step()  # update learning rate schedule \n",
    "\n",
    "            # log losses\n",
    "            loss /= len(dataloader_train)\n",
    "            losses += loss.item()\n",
    "            \n",
    "        # output stats\n",
    "        print(f\"Epoch {e}; loss {losses}\")\n",
    "        log(model_dir, \"Epoch \" + str(e+1) + \"; loss \" + str(losses), log_file)\n",
    "        all_losses.append(losses)\n",
    "        # save model\n",
    "        __save_model(model_dir, model) # save latest\n",
    "        if (e > current_epoch and losses < all_losses[-1]):\n",
    "            __save_model(model_dir, model, model_type='best') # save best model        \n",
    "        \n",
    "        \n",
    "\n",
    "def test(model, dataloader_test, model_dir, log_file):\n",
    "    \n",
    "    model, e = __load_experiment(model_dir, model, model_type='latest')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    losses = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader_test), total=len(dataloader_test)):\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['input_mask'].to(device),\n",
    "#             decoder_input_ids=batch['target_ids'].to(device),\n",
    "#             decoder_attention_mask=batch['target_mask'].to(device),\n",
    "            start_positions = batch['start'].to(device), \n",
    "            end_positions = batch['end'].to(device)\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # log losses\n",
    "        loss /= len(dataloader_test)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    # output stats\n",
    "    print(f\"Validation loss {losses}\")\n",
    "    log(model_dir, \"Validation loss \" + str(losses), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42d33bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, lr, eps): \n",
    "    if opt_name == 'Adam':\n",
    "        return AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "    elif opt_name == 'SGD':\n",
    "        return SGD(model.parameters(), lr=lr, eps=eps)\n",
    "    \n",
    "def get_scheduler(model, scheduler, n_batches, n_epochs, warmup_portion=0.1):\n",
    "    train_steps = n_epochs*n_batches\n",
    "    warm_step = int(train_steps*warmup_portion)\n",
    "    if scheduler == \"linear\": \n",
    "        return get_linear_schedule_with_warmup(model.optimizer, num_warmup_steps=warm_step,num_training_steps=train_steps)\n",
    "    elif scheduler == \"cosine\":\n",
    "        return get_cosine_schedule_with_warmup(model.optimizer, num_warmup_steps=warm_step,num_training_steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44ef459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train feature data with 2259 batches\n",
      "Loaded test feature data with 425 batches\n",
      "roberta-base-squad2\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "n_epochs = 10\n",
    "lr = 1e-5\n",
    "weight_decay = 5e-5\n",
    "batch_size = 1\n",
    "\n",
    "# dataloaders\n",
    "# default split point: 425 -> samples after the split point will be in the test set\n",
    "dataloader_train, dataloader_test = get_dataloaders(train_dataset, test_dataset, batch_size=batch_size)\n",
    "\n",
    "# model optimizer\n",
    "model.optimizer = get_optimizer(model, \"Adam\", lr, weight_decay)\n",
    "\n",
    "# learning rate scheduler\n",
    "model.scheduler = get_scheduler(model, \"linear\", len(dataloader_train), n_epochs)\n",
    "\n",
    "name = checkpoint.split('/')[-1]\n",
    "print(name)\n",
    "\n",
    "# model state_dict\n",
    "model_dir = f\"{name}_e{n_epochs}_lr{lr}_eps{weight_decay}_Adam_linearS_batch{batch_size}\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# log file\n",
    "log_file = \"train.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea487f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was Abraham Lincoln the sixteenth President of the United States?\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "question = raw_dataset[0][0]\n",
    "text = raw_dataset[0][1]\n",
    "print(question)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cadb789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; loss 0.002366676836057735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; loss 5.1048086130656145e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2; loss 1.8201642551174402e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3; loss 8.83299624954148e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4; loss 4.7231861232122085e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2259/2259 [02:01<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5; loss 2.9641582992567805e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████▉                            | 1455/2259 [01:18<00:43, 18.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader_train, n_epochs, model_dir, log_file)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# log losses\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader_train)\n\u001b[0;32m---> 37\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# output stats\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader_train, n_epochs, model_dir, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2379814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 425/425 [00:04<00:00, 88.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 6.215710557366805e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloader_test, model_dir, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975f7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 3.797605GB\n",
      "torch.cuda.memory_reserved: 15.416016GB\n",
      "torch.cuda.max_memory_reserved: 15.416016GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7265f0",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bc8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the experiment data if exists to resume training from last saved checkpoint.\n",
    "def __load_experiment(model_dir, model, model_type='latest'):\n",
    "    \n",
    "    if model_type == \"latest\":\n",
    "        saved_name = 'latest_model.pt'\n",
    "    else:\n",
    "        saved_name = 'best_model.pt'\n",
    "\n",
    "    if os.path.exists(os.path.join(model_dir, 'train.log')):\n",
    "        # get the latest model\n",
    "        state_dict = torch.load(os.path.join(model_dir, saved_name), map_location=device.type)\n",
    "        model.load_state_dict(state_dict['weights'])\n",
    "    else: \n",
    "        print(\"model state dict doesn't exist\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bc447b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_dir = 'roberta-base-squad2_e10_lr1e-05_eps5e-05_Adam_linearS_batch1'\n",
    "model = __load_experiment(model_dir, model, model_type='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8fd35c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e754176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0864cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=processer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a7eedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Giraffes can inhabit savannas, grasslands, or open woodlands. They prefer areas enriched with acacia growth. They drink large quantities of water and, as a result, they can spend long periods of time in dry, arid areas. When searching for more food they will venture into areas with denser foliage.\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4baee174",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What type of vegetation do giraffes prefer?'\n",
    "answer = 'acacia' # gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "964192f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is the bear white?\"\n",
    "context = \"a white bear running to a tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f07aa5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1349cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 3.3720368924699488e-15,\n",
       " 'start': 2,\n",
       " 'end': 12,\n",
       " 'answer': 'white bear'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78554459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question_answerer2 = pipeline(\"question-answering\", model='t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0f434a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.023177172988653183,\n",
       " 'start': 0,\n",
       " 'end': 25,\n",
       " 'answer': 'a white bear running to a'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer2(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c754f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer3 = pipeline(\"question-answering\", model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50087e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1231619194149971, 'start': 0, 'end': 7, 'answer': 'a white'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer3(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850e78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
